{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction Project using Azure AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801499354
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Import libraries needed\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "import pkg_resources\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801508011
    }
   },
   "outputs": [],
   "source": [
    "#Testing the authentication using the Workspace method \"from_config\"\n",
    "Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801518339
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#create workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801525179
    }
   },
   "outputs": [],
   "source": [
    "# Create Experiment\n",
    "\n",
    "experiment_name = 'Titanic-AutoML-Experiment'\n",
    "project_folder = './Titanic-project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview:\n",
    "\n",
    "As you probably have guessed from the project title we will be working with the \"Titanic Dataset\" which is already a classical dataset to learn Machine Learning.\n",
    "\n",
    "The main task for this project will be to build a predictive model that answers the question: “what sorts of people were more likely to survive?” To answer the above stated question we are going to give the model different input variables such as age, type of cabin the passanger had, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801553547
    }
   },
   "outputs": [],
   "source": [
    "#Getting the dataset\n",
    "key = \"titanic_dataset\"\n",
    "description_text = \"Titanic Dataset for Model Deployment\"\n",
    "\n",
    "dataset = TabularDatasetFactory.from_delimited_files('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')\n",
    "\n",
    "#Register Dataset and it will store into the Datasets->Registered Dataset\n",
    "dataset = dataset.register(workspace=ws,\n",
    "                           name=key,\n",
    "                           description=description_text)\n",
    "\n",
    "#Create a df out of the registered dataset\n",
    "dataset = dataset.to_pandas_dataframe()\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801571099
    }
   },
   "outputs": [],
   "source": [
    "###Be sure to have the compute target setup\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"compute-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',# for GPU, use \"STANDARD_NC6\"\n",
    "                                                           #vm_priority = 'lowpriority', # optional\n",
    "                                                           min_nodes=1,\n",
    "                                                           max_nodes=6)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801584314
    }
   },
   "outputs": [],
   "source": [
    "#Import your clean data function from the train.py file\n",
    "from train import clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801590391
    }
   },
   "outputs": [],
   "source": [
    "#apply the function\n",
    "x, y = clean_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801597126
    }
   },
   "outputs": [],
   "source": [
    "#Scale the features\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801601773
    }
   },
   "outputs": [],
   "source": [
    "# create scaler\n",
    "variables = x.columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x[variables]) \n",
    "\n",
    "x = scaler.transform(x[variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801607638
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(x,columns=variables)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801614270
    }
   },
   "outputs": [],
   "source": [
    "#Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3) # 30% as test data,random_state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801620929
    }
   },
   "outputs": [],
   "source": [
    "#bring them together them again\n",
    "dataset = pd.concat([x_train,y_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801626363
    }
   },
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801633010
    }
   },
   "outputs": [],
   "source": [
    "#To train the model we need a TabularDataset and not a dataframe, therefore the current df will be converterd \n",
    "#into a TabularDataset:\n",
    "\n",
    "#Convert the dataframe into a csv\n",
    "local_path = 'prepared.csv'\n",
    "\n",
    "#Save it locally\n",
    "dataset.to_csv(local_path,index=None)\n",
    "\n",
    "#Generate the a datastore object which is the the default datastore\n",
    "datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801636555
    }
   },
   "outputs": [],
   "source": [
    "#Upload the dataframe which was previosly converted into a csv\n",
    "datastore.upload(src_dir='.', target_path='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801641127
    }
   },
   "outputs": [],
   "source": [
    "#For the sake of checking; check the path\n",
    "datastore.path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801648178
    }
   },
   "outputs": [],
   "source": [
    "#Now the uploaded file will be transformed into a Tabular dataset and store in a varible named 'training_dataset'\n",
    "training_dataset = Dataset.Tabular.from_delimited_files(\"https://raw.githubusercontent.com/ddgope/Titanic-Survival-Prediction/master/prepared.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801654940
    }
   },
   "outputs": [],
   "source": [
    "#let's visualize the data:\n",
    "training_dataset.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "Below we will chose the automl settings and cofiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612801675790
    }
   },
   "outputs": [],
   "source": [
    "#Create the automl settings which will be used as argurments in the automl config\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 20,\n",
    "    \"max_concurrent_iterations\": 5,\n",
    "    \"primary_metric\" : 'accuracy'\n",
    "}\n",
    "\n",
    "#Create the automl_config\n",
    "automl_config = AutoMLConfig(compute_target=compute_target,\n",
    "                             task = \"classification\",\n",
    "                             training_data=training_dataset,\n",
    "                             label_column_name=\"survived\",   \n",
    "                             path = project_folder,\n",
    "                             enable_early_stopping= True,\n",
    "                             featurization= 'auto',\n",
    "                             debug_log = \"automl_errors.log\",\n",
    "                             **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612716689612
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Submitt the experiment\n",
    "automl_run = experiment.submit(automl_config,show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612802994779
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Additional Run Details\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(automl_run).show()\n",
    "\n",
    "# wait for completion\n",
    "automl_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612803039483
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Get generic outputs from the automl_run\n",
    "automl_run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612803063173
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Get the best model outputs\n",
    "best_automl_run, best_model = automl_run.get_output()\n",
    "\n",
    "\n",
    "# Retrieve the best automl run model\n",
    "print('Best AutoML run: ', best_automl_run)\n",
    "print('Best AutoML model :', best_model)\n",
    "\n",
    "# get best model and display properties\n",
    "model_name = best_automl_run.properties['model_name']\n",
    "print('Best_model name: ', model_name)\n",
    "\n",
    "# display all the properties of the best model\n",
    "best_automl_run.get_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612803097305
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Get the best model id\n",
    "print(best_automl_run.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the best AutoML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612803119090
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(best_model, 'best_automl_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML Studio\n",
    "\n",
    "I will be using the azurenl sdk for Python for depoyment.\n",
    "Ref: https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install azureml , azureml-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Prepare deploying of the model as a web service\n",
    "# from azureml.core import Workspace\n",
    "# from azureml.core import Workspace\n",
    "# from azureml.core.model import Model\n",
    "\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "#from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my case Workspace has alreday been created in top otherwise. \n",
    "# Need to create using below command\n",
    "\n",
    "# ws = Workspace.create(\n",
    "#                name='myworkspace',            \n",
    "#                subscription_id='<azure-subscription-id>',           \n",
    "#                resource_group='myresourcegroup',                 \n",
    "#                create_resource_group=True,                 \n",
    "#                location='eastus2'                \n",
    "#                )\n",
    "\n",
    "#Once it is created, save the details of the workspace in a config.json \n",
    "#file to use the workspace later.\n",
    "# ws.write_config()\n",
    "\n",
    "# Create a folder named ‘.azureml’ and save the json file inside it. \n",
    "# Make sure you name the folder and json file correctly.\n",
    "# Python will look for the config.json file inside this folder \n",
    "# while loading up your workspace\n",
    "\n",
    "# You won’t need to create a workspace every time, in the future, \n",
    "# you can use the following command to load the workspace\n",
    "# #ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the Model\n",
    "1. workspace: The workspace object we created\n",
    "2. model_path: the path to the pickle file\n",
    "3. model_name: Name of the model on Azure MLS\n",
    "4. tags: Although not necessary, you can add tags to your model\n",
    "5. description: A description of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612803144972
    }
   },
   "outputs": [],
   "source": [
    "# register best AutoML model for future deployment\n",
    "from azureml.core.model import Model\n",
    "description = 'AutoML Model trained on the titanic dataset'\n",
    "tags = {'area': 'data science beginners', 'type': 'classification'}\n",
    "\n",
    "automl_model = Model.register(workspace =ws,\n",
    "                              model_name = 'best-titanicMLmodel',\n",
    "                              model_path = 'best_automl_model.pkl',\n",
    "                             description = description, tags = tags)\n",
    "\n",
    "print('AutoML RunID: ', automl_run.id, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Environment\n",
    "Environment was alreday created otherwise create using below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to install required packages\n",
    "# env = Environment('env')\n",
    "# cd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0'], conda_packages = ['scikit-learn==0.23.2'])\n",
    "# env.python.conda_dependencies = cd\n",
    "## Register environment to re-use later\n",
    "# env.register(workspace = ws)\n",
    "# print(\"Registered Environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612806348039
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#get’s the environment whuich just created or previously created\n",
    "env = Environment.get(workspace=ws, name='AzureML-AutoML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm that all the required libraries have been installed by creating a .yml \n",
    "#based on the created environment. Ensure all the libraries are mentioned inside the .yml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will create a new folder called environ with a .yml and a .json file inside it\n",
    "env.save_to_directory('./environ', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612806352240
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Chekc environment dependencies\n",
    "print(\"packages\", env.python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Objects\n",
    "Create a container instance and set the number of cpu_cores and memory_gb based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_config = AciWebservice.deploy_configuration(\n",
    "            cpu_cores=1,\n",
    "            memory_gb=4, \n",
    "            enable_app_insights=True,\n",
    "            auth_enabled=True,\n",
    "            tags={\"data\":\"titanic classifier\"},\n",
    "            description='titanic classification Model',\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612806374855
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Create and InferenceConfig instance to link the environment and entry script. \n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "\n",
    "#In my case Entry Script is already create \n",
    "#and stored in the path otherwise you can create using below command\n",
    "#The entry script will two functions, an init function and a run function.\n",
    "#     Create a global variable called model\n",
    "#     Load the model using the name of the model you registered earlier\n",
    "#     Load the model from the path\n",
    "# def init():\n",
    "#     global modelmodel_path = Model.get_model_path(\"knn\")\n",
    "#     print(\"Model Path is  \", model_path)\n",
    "#     model = joblib.load(model_path)\n",
    "\n",
    "# #Save this file.\n",
    "# def run(data):\n",
    "#     try:\n",
    "#        print(data)\n",
    "#        result = model.predict(data['data']) \n",
    "#        return {'data' : result.tolist() , 'message' : \"Successfully  classified Titanic\"}\n",
    "#     except Exception as e:\n",
    "#            error = str(e)\n",
    "#            return {'data' : error , 'message' : 'Failed to classify Titanic'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612806525913
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#deploy the model by combining our config objects, workspace and model together. \n",
    "service_name = 'my-ml-service'\n",
    "\n",
    "model = Model(ws,name='best-titanicMLmodel')\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=aci_config,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612806543093
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#deployment is successful, the rest endpoint will be printed out.\n",
    "# If it is unsuccessful, see the deployment logs\n",
    "service.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612806554530
    }
   },
   "outputs": [],
   "source": [
    "# print service state\n",
    "print(service.state)\n",
    "# print scoring URI\n",
    "print('scoring URI: ' + service.scoring_uri)\n",
    "# print Swagger URI\n",
    "print('Swagger URI: ' + service.swagger_uri)\n",
    "# retrieve authentication keys\n",
    "primary, secondary = service.get_keys()\n",
    "# print primary authenticaton key\n",
    "print('Primary Authentication Key: ' + primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612806747921
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Store the uri's in variables:\n",
    "scoring_uri = 'http://87897773-cb15-40d5-ba0d-ba8285d8f467.southcentralus.azurecontainer.io/score'\n",
    "\n",
    "key = 'iBX2glUB3xcahOndX5AW62WoVbRiDcIZ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consume the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612814619158
    }
   },
   "outputs": [],
   "source": [
    "#let's test requests:\n",
    "import json\n",
    "import requests\n",
    "\n",
    "scoring_uri = scoring_uri\n",
    "key = key\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "headers['Authorization'] = f'Bearer {key}'\n",
    "\n",
    "\n",
    "test_data = json.dumps({'data':[{\n",
    "    'pclass': 0.8419164182590155,\n",
    "    'age': -0.34907541344456255,\n",
    "    'sibsp': -0.47908676070718687,\n",
    "    'parch': -0.444999501816175,\n",
    "    'fare': -0.4902404567566683,\n",
    "    'age_NA': -0.5014319838391105,\n",
    "    'fare_NA': -0.027650063180466557,\n",
    "    'sex_male': 0.743496915331831,\n",
    "    'cabin_Missing': 0.5393765119990418,\n",
    "    'cabin_Rare': -0.42592011250734235,\n",
    "    'embarked_Q': -0.32204029159373954,\n",
    "    'embarked_Rare': -0.03911805059269843,\n",
    "    'embarked_S': 0.6573935670276714,\n",
    "    'title_Mr': 0.8525918887485938,\n",
    "    'title_Mrs': -0.42592011250734235,\n",
    "    'title_Rare': -0.27494677157229536\n",
    "    }\n",
    "    ]\n",
    "        })\n",
    "\n",
    "test_data2 = json.dumps({'data':[{\n",
    "    'pclass': -15460978645168200,\n",
    "    'age': 0.8912042887450313,\n",
    "    'sibsp': -0.47908676070718687,\n",
    "    'parch': -0.444999501816175,\n",
    "    'fare': 19569900306355100,\n",
    "    'age_NA': -0.5014319838391105,\n",
    "    'fare_NA': -0.027650063180466557,\n",
    "    'sex_male': -13449954927569300,\n",
    "    'cabin_Missing': -18539924853119600,\n",
    "    'cabin_Rare': 23478581326275300,\n",
    "    'embarked_Q': -0.32204029159373954,\n",
    "    'embarked_Rare': -0.03911805059269843,\n",
    "    'embarked_S': -15211587854766800,\n",
    "    'title_Mr': -11728941046668400,\n",
    "    'title_Mrs': -0.42592011250734235,\n",
    "    'title_Rare': -0.27494677157229536\n",
    "\n",
    "    }\n",
    "    ]\n",
    "        })\n",
    "\n",
    "\n",
    "response1 = requests.post(scoring_uri, data=test_data, headers=headers)\n",
    "response2 = requests.post(scoring_uri, data=test_data2, headers=headers)\n",
    "\n",
    "print(\"Classification Prediction:\",response1.text)\n",
    "print(\"Classification Prediction:\",response2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612811909768
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#get the environment Details and stored them into a file:\n",
    "f = open(\"env.yml\", \"w\")\n",
    "f.write(env.python.conda_dependencies.serialize_to_string())\n",
    "f.close()\n",
    "\n",
    "print(\"packages\", env.python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Delete Service:\n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
